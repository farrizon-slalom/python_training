{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAl-YipkvOtC"
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# Welcome to Module 6: Data Analysis in Pandas\n",
    "\n",
    "# -- Contents --\n",
    "# 1. DATA DESCRIPTIONS AND AGGREGATIONS\n",
    "# 2. GROUPED AGGREGATIONS\n",
    "# 3. SORTING AND FILTERING\n",
    "# 4. TABLE JOINS\n",
    "# 4. ADVANCED STATISTICAL METHODS\n",
    "# 5. TRANSFORMING DATAFRAMES\n",
    "# 6. VISUALIZING DATA\n",
    "# 7. ACTIVITIES\n",
    "\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu4oSL5B5Xpt"
   },
   "source": [
    "\n",
    "########################\n",
    "# 1. DATA DESCRIPTIONS AND AGGREGATIONS\n",
    "########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW_W28pf5bGY"
   },
   "source": [
    "##########\n",
    "## 1.1 SETUP\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXHlZ23O5SWH"
   },
   "outputs": [],
   "source": [
    "# First, we'll import several libraries\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "8X8RV_zJ5e97",
    "outputId": "756f5bea-02cf-4100-afda-f2c24b8de151"
   },
   "outputs": [],
   "source": [
    "# Then, we'll load the 2 dataframes that we'll be working with\n",
    "# The first is the same sales dataset that we worked with in Module 5\n",
    "pickle_file_path = './data/sales_df.pickle'\n",
    "with open(pickle_file_path, 'rb') as handle:\n",
    "    sales_df = pickle.load(handle)\n",
    "\n",
    "# The second is a customer dataset that has customer information\n",
    "pickle_file_path = './data/customers_data.pickle'\n",
    "with open(pickle_file_path, 'rb') as handle:\n",
    "    customer_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "810xuGeL5kse"
   },
   "outputs": [],
   "source": [
    "# Also the same as Module 5, we'll force Pandas to show all of the columns when printing\n",
    "pd.set_option('display.max_columns', 1000) # Now we will see up to 1000 columns\n",
    "pd.set_option('display.max_colwidth', 1000) # And we will see up to 1000 characters in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv('./data/sales_data_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9iz0m3v1Kfa"
   },
   "outputs": [],
   "source": [
    "customer_df = pd.read_csv('./data/customers_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipAlec9z6XHm"
   },
   "source": [
    "##########\n",
    "## 1.2 DATA DESCRIPTIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "colab_type": "code",
    "id": "ivXtIy-FvOtG",
    "outputId": "b7760d7b-604f-4549-8080-1405261f5c6c"
   },
   "outputs": [],
   "source": [
    "# We can use the .describe() function to view some general statistics about each column in a dataframe\n",
    "# Let's look at the sales dataframe\n",
    "sales_description = sales_df.describe()\n",
    "sales_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "xPJ26ce_6lkS",
    "outputId": "17504d95-35e2-47d2-820e-54a2169a055b"
   },
   "outputs": [],
   "source": [
    "# Now let's look at the customer dataframe\n",
    "cust_description = customer_df.describe()\n",
    "cust_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "s-2YNfyt6sTG",
    "outputId": "47b0ee7a-0acb-4d7c-cac2-3b00c2715dbd"
   },
   "outputs": [],
   "source": [
    "# To determine the data type of each column, we can use the .dtypes function\n",
    "# Note: Strings will show as 'object'\n",
    "dtypes = sales_df.dtypes\n",
    "dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYr9Zqdj6xeC"
   },
   "source": [
    "##########\n",
    "## 1.3 AGGREGATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "KDkfhxCw6uo2",
    "outputId": "19c5d220-c9f7-4967-daa6-02c9d6099221"
   },
   "outputs": [],
   "source": [
    "# There are many built-in aggregation function we can use to compute individual column statistics\n",
    "# These include, but are not limited to: count, sum, mean, median, mode, min, max, abs, prod, std, var \n",
    "# Note: Some of these statistics are the same as what we see when we use the .describe() function\n",
    "\n",
    "# We can use .count() to tell us the number of rows in each column containing non-NA values\n",
    "count = sales_df.count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "oQqYkAiq60hY",
    "outputId": "88aa15e3-f1b9-4ba0-824b-b0747325b72a"
   },
   "outputs": [],
   "source": [
    "# For all of these aggregation functions, we can also apply them to a subset of columns\n",
    "selection = sales_df[['customer_key', 'product_key']]\n",
    "count = selection.count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qgpj5v1p61_C",
    "outputId": "843bbbac-a4f9-42f0-c533-ca86d23c3f09"
   },
   "outputs": [],
   "source": [
    "# We can use .sum() to sum all of the values in each column\n",
    "selection = sales_df['product_cost']\n",
    "my_sum = selection.sum() # Note: We cannot call our variable 'sum' because that is a reserved Python keyword\n",
    "print(my_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "WBg28VFJ66PQ",
    "outputId": "df4892b3-2dd5-42c3-8f3d-2c5036739d8b"
   },
   "outputs": [],
   "source": [
    "# We can also use the .agg() function to compute multiple statistics on 1 or more columns at once\n",
    "# Note: This is similar to what the .describe() function does, but it allows us to specify the stats we want\n",
    "stats = sales_df['product_cost'].agg(['count', 'sum', 'mean', 'std', 'min', 'max'])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aczcUyhS7cwo"
   },
   "source": [
    "########################\n",
    "# 2. GROUPED AGGREGATIONS\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxY-KWii7edp"
   },
   "source": [
    "##########\n",
    "## 2.1 BASIC GROUPED AGGREGATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "GS_vDfL3vOtJ",
    "outputId": "bd4c08a5-8a4d-4c5e-ae9f-d38766c3169e"
   },
   "outputs": [],
   "source": [
    "# Rather than computing column statistics that consider each row individually, \n",
    "#   we can group rows based on one of their column values using the .groupby() function\n",
    "# Then, we can compute statistics for each group\n",
    "\n",
    "# We'll start with a simple example: grouping and computing a single metric\n",
    "grouped_data = sales_df.groupby('product_name')\n",
    "count = grouped_data['order_quantity'].agg('count') # Count number of rows in each group with non-NA values for 'OrderQuantity'\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "iodxQzJg7l0q",
    "outputId": "75891b5f-797e-4f32-b026-75d620f695d2"
   },
   "outputs": [],
   "source": [
    "# We can accomplish the same thing in a more compact way\n",
    "count = sales_df.groupby('product_name')['order_quantity'].agg('count')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "NKMNXAqT7pvg",
    "outputId": "caf21d42-9d84-40ec-afd5-919d0489b8f8"
   },
   "outputs": [],
   "source": [
    "# Similar to what we saw in the previous section, we can compute multiple metrics at once\n",
    "# Group and compute\n",
    "stats = sales_df.groupby('product_name')['order_quantity'].agg(['count', 'sum', 'mean', 'std', 'min', 'max']) \n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj_drfxK7uev"
   },
   "outputs": [],
   "source": [
    "# We can use .reset_index() to move the 'product_name' from the index back to a regular column,\n",
    "# and set the new row numbers as the index.\n",
    "stats = stats.reset_index()\n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "1. Find the average product_cost for each product_name.\n",
    "2. Find the sum of product_cost for 'AWC Logo Cap'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OsSdesNI7xhP"
   },
   "source": [
    "##########\n",
    "## 2.2 ADVANCED GROUPED AGGREGATIONS\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odoLdftJvOtL"
   },
   "outputs": [],
   "source": [
    "# For more complex aggregations, it's best to use your own aggregation function\n",
    "\n",
    "# Here, we define a function that computes the 90th percentile value for a given data series\n",
    "import numpy as np\n",
    "def ninetieth_percentile(x):\n",
    "  p = np.percentile(x, 90)\n",
    "  return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "bskJp1qp71Jf",
    "outputId": "a1a204da-31f4-4c2b-c450-503be54d4989"
   },
   "outputs": [],
   "source": [
    "# Then, we can supply the function name as an input in the .agg() function\n",
    "stats = sales_df.groupby('category_name')['product_price'] .agg(['min', ninetieth_percentile, 'max']).reset_index() \n",
    "stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsykvRvO7_Qe"
   },
   "source": [
    "########################\n",
    "# 3. SORTING AND FILTERING\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wcubKu9g8AbZ"
   },
   "source": [
    "##########\n",
    "## 3.1 SORTING\n",
    "##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "6E-SZ7wdvOtO",
    "outputId": "afcb3558-7930-4aa1-8de9-65b1a6086e73",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can sort rows using the .sort_values() function\n",
    "# Note: If sorting using multiple columns, the function first sorts by the left-most column provided\n",
    "#       Then, rows are further sorted by the next column provided, etc.\n",
    "#       This can handle cases where some column values are the same.\n",
    "sorted_df = sales_df.sort_values(by = ['stock_date', 'product_name', 'product_size'], ascending=True) \n",
    "sorted_selection = sorted_df[['stock_date', 'product_name', 'product_size', 'model_name']]\n",
    "sorted_selection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = sorted_df.reset_index(drop = True)\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "colab_type": "code",
    "id": "bFiAIP2SvOtR",
    "outputId": "6fdc91ad-119c-48b7-d5da-6b7252964e43"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 3.2 FILTERING\n",
    "##########\n",
    "# Filtering rows is the equivalent of using `WHERE` in SQL.\n",
    "# Note: Filtering occurs by requesting rows where a condition is `True`.\n",
    "\n",
    "# First, we need to build a series of True/False values for each row \n",
    "# The value is true for each row where column 'CategoryName' has value 'Bikes'\n",
    "wanted_category_name = 'Bikes' \n",
    "rows_tf = (sales_df['category_name'] == wanted_category_name)\n",
    "rows_tf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we use the True/False series to filter the dataframe\n",
    "# The result of this funtion is a new dataframe with all of the rows that meet our filter condition\n",
    "filtered_df = sales_df[rows_tf]\n",
    "print('The original dataframe has %d rows' % len(sales_df))\n",
    "print('The filtered dataframe has %d rows' % len(filtered_df))\n",
    "filtered_df[['product_name', 'model_name', 'product_style']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Find all rows with product cost > 2000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can accomplish the same filtering in a more compact way\n",
    "wanted_category_name = 'Bikes' \n",
    "columns_wanted = ['product_name', 'model_name', 'product_style', 'subcategory_name']\n",
    "filtered_df = sales_df[sales_df['category_name'] == wanted_category_name][columns_wanted]\n",
    "\n",
    "print('The filtered dataframe has %d rows' % len(filtered_df))\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also filter using compound conditions\n",
    "wanted_category_name = 'Bikes' \n",
    "subcategory_name = 'Mountain Bikes'\n",
    "columns_wanted = ['product_name', 'model_name', 'product_style', 'subcategory_name']\n",
    "rows_tf = (sales_df['category_name'] == wanted_category_name) & (sales_df['subcategory_name'] == subcategory_name)\n",
    "\n",
    "filtered_df = sales_df[rows_tf][columns_wanted]\n",
    "print('The filtered dataframe has %d rows' % len(filtered_df))\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Find all rows with category = Bikes, subcategory_name == Mountain Bikes, and product_cost between 1500 and 2000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another type of filtering is considering only unique values, using the .drop_duplicates() function\n",
    "# Here, we first sort the dataframe using the 'ProductName' and 'ModelName' columns\n",
    "\n",
    "sorted_df = sales_df.sort_values(by = ['product_name', 'model_name'], ascending=True) \n",
    "print(sorted_df[['product_name', 'model_name', 'product_style']].head())\n",
    "\n",
    "# Then, we drop all rows containing duplicate values in columns 'ProductName' and 'ModelName'\n",
    "unique_sorted_df = sorted_df.drop_duplicates(['product_name', 'model_name']) \n",
    "\n",
    "# Now, let's look at the result\n",
    "unique_sorted_df[['product_name', 'model_name', 'product_style']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sorted_df[['product_name', 'model_name', 'product_style']].reset_index(drop = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "PF0yWbh4vOtU",
    "outputId": "1d01ab23-e385-4fd5-8949-84cd372f643b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# 4. TABLE JOINS\n",
    "########################\n",
    "\n",
    "##########\n",
    "# 4.1 GETTING STARTED\n",
    "##########\n",
    "\n",
    "# Altough the  sales data (sales_df) has more rows than the customer data (customer_df), \n",
    "# the former has fewer unique customer_key's than the latter.\n",
    "\n",
    "# inner and outer joins\n",
    "\n",
    "# Sales data\n",
    "print(len(sales_df['customer_key']))\n",
    "print(len(sales_df['customer_key'].drop_duplicates()))\n",
    "\n",
    "# Customer data\n",
    "print(len(customer_df['customer_key']))\n",
    "print(len(customer_df['customer_key'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.loc[:5, 'customer_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.loc[:5, 'customer_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(sales_df.columns.to_list())\n",
    "y = np.asarray(customer_df.columns.to_list())\n",
    "\n",
    "np.intersect1d(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "eUoWdepcvOtX",
    "outputId": "730d3b53-337a-4237-8b46-1abcc5966523",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 4.2 INNER JOIN\n",
    "##########\n",
    "\n",
    "# Get the intersection of the Sales and customer entries with an identical customer_key\n",
    "\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df,                 # Left and right dataframes\n",
    "                        how='inner',                                      # Specify inner join\n",
    "                        left_on='customer_key', right_on='customer_key',  # Join keys\n",
    "                        suffixes=('_sales', '_cust')                      # To apply to overlapping column names\n",
    "                    )\n",
    "\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "MhhfSkn0vOtZ",
    "outputId": "c2c3095b-7214-4a47-f321-9708f8c8d7cd"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 4.3 LEFT JOIN\n",
    "##########\n",
    "\n",
    "# \"Complement\" the Sales data with the Customer dataset\n",
    "# Some sales entries do not have a counterpart in the cust data. Look at the `_merge` column added by the \n",
    "# argument 'indicator=True' to see which rows from the sales data did not have a counterpart in the customer dataset\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df, \n",
    "                        how='left',\n",
    "                        left_on='customer_key', right_on='customer_key',\n",
    "                        suffixes=('_sales', '_cust'),\n",
    "                        indicator=True)\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "toKmtPNHvOtc",
    "outputId": "f2b41409-71ff-4558-e345-37456e99b596",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 4.4 RIGHT JOIN\n",
    "##########\n",
    "\n",
    "# For each entry in the customer dataset, find all the sales which match on customer_key\n",
    "# Look at the _merge column added by the argument 'indicator=True' \n",
    "#  to see which customer rows did not have a counterpart in the sales dataset.\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df, \n",
    "                        how='right',\n",
    "                        left_on='customer_key', right_on='customer_key',\n",
    "                        suffixes=('_sales', '_cust'),\n",
    "                        indicator=True)\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "732\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      order_date stock_date order_number  product_key  customer_key  \\\n56046        NaN        NaN          NaN          NaN         11051   \n56047        NaN        NaN          NaN          NaN         11133   \n56048        NaN        NaN          NaN          NaN         11349   \n56049        NaN        NaN          NaN          NaN         11435   \n56050        NaN        NaN          NaN          NaN         11441   \n\n       territory_key  order_line_item  order_quantity  \\\n56046            NaN              NaN             NaN   \n56047            NaN              NaN             NaN   \n56048            NaN              NaN             NaN   \n56049            NaN              NaN             NaN   \n56050            NaN              NaN             NaN   \n\n       product_subcategory_key product_sku product_name model_name  \\\n56046                      NaN         NaN          NaN        NaN   \n56047                      NaN         NaN          NaN        NaN   \n56048                      NaN         NaN          NaN        NaN   \n56049                      NaN         NaN          NaN        NaN   \n56050                      NaN         NaN          NaN        NaN   \n\n      product_description product_color product_size product_style  \\\n56046                 NaN           NaN          NaN           NaN   \n56047                 NaN           NaN          NaN           NaN   \n56048                 NaN           NaN          NaN           NaN   \n56049                 NaN           NaN          NaN           NaN   \n56050                 NaN           NaN          NaN           NaN   \n\n       product_cost  product_price subcategory_name  product_category_key  \\\n56046           NaN            NaN              NaN                   NaN   \n56047           NaN            NaN              NaN                   NaN   \n56048           NaN            NaN              NaN                   NaN   \n56049           NaN            NaN              NaN                   NaN   \n56050           NaN            NaN              NaN                   NaN   \n\n      category_name prefix first_name last_name birth_date marital_status  \\\n56046           NaN    MR.     DANIEL   JOHNSON   8/4/1951              S   \n56047           NaN    MS.     ANGELA   GRIFFIN   9/8/1980              S   \n56048           NaN   MRS.      MINDY       LUO   9/3/1954              M   \n56049           NaN   MRS.      ROBIN    ROMERO   3/6/1950              S   \n56050           NaN   MRS.      ERIKA     GOMEZ  6/12/1947              M   \n\n      gender                 email_address annual_income  total_children  \\\n56046      M  daniel18@adventure-works.com      $30,000                3   \n56047      F  angela23@adventure-works.com      $30,000                0   \n56048      F   mindy10@adventure-works.com      $10,000                2   \n56049      F    robin5@adventure-works.com     $100,000                3   \n56050      F    erika0@adventure-works.com      $90,000                5   \n\n           education_level      occupation home_owner      _merge  \n56046          High School  Skilled Manual          N  right_only  \n56047  Partial High School        Clerical          N  right_only  \n56048      Partial College          Manual          Y  right_only  \n56049      Partial College      Management          Y  right_only  \n56050  Partial High School  Skilled Manual          N  right_only  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_date</th>\n      <th>stock_date</th>\n      <th>order_number</th>\n      <th>product_key</th>\n      <th>customer_key</th>\n      <th>territory_key</th>\n      <th>order_line_item</th>\n      <th>order_quantity</th>\n      <th>product_subcategory_key</th>\n      <th>product_sku</th>\n      <th>product_name</th>\n      <th>model_name</th>\n      <th>product_description</th>\n      <th>product_color</th>\n      <th>product_size</th>\n      <th>product_style</th>\n      <th>product_cost</th>\n      <th>product_price</th>\n      <th>subcategory_name</th>\n      <th>product_category_key</th>\n      <th>category_name</th>\n      <th>prefix</th>\n      <th>first_name</th>\n      <th>last_name</th>\n      <th>birth_date</th>\n      <th>marital_status</th>\n      <th>gender</th>\n      <th>email_address</th>\n      <th>annual_income</th>\n      <th>total_children</th>\n      <th>education_level</th>\n      <th>occupation</th>\n      <th>home_owner</th>\n      <th>_merge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>56046</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11051</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MR.</td>\n      <td>DANIEL</td>\n      <td>JOHNSON</td>\n      <td>8/4/1951</td>\n      <td>S</td>\n      <td>M</td>\n      <td>daniel18@adventure-works.com</td>\n      <td>$30,000</td>\n      <td>3</td>\n      <td>High School</td>\n      <td>Skilled Manual</td>\n      <td>N</td>\n      <td>right_only</td>\n    </tr>\n    <tr>\n      <th>56047</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11133</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MS.</td>\n      <td>ANGELA</td>\n      <td>GRIFFIN</td>\n      <td>9/8/1980</td>\n      <td>S</td>\n      <td>F</td>\n      <td>angela23@adventure-works.com</td>\n      <td>$30,000</td>\n      <td>0</td>\n      <td>Partial High School</td>\n      <td>Clerical</td>\n      <td>N</td>\n      <td>right_only</td>\n    </tr>\n    <tr>\n      <th>56048</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11349</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MRS.</td>\n      <td>MINDY</td>\n      <td>LUO</td>\n      <td>9/3/1954</td>\n      <td>M</td>\n      <td>F</td>\n      <td>mindy10@adventure-works.com</td>\n      <td>$10,000</td>\n      <td>2</td>\n      <td>Partial College</td>\n      <td>Manual</td>\n      <td>Y</td>\n      <td>right_only</td>\n    </tr>\n    <tr>\n      <th>56049</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11435</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MRS.</td>\n      <td>ROBIN</td>\n      <td>ROMERO</td>\n      <td>3/6/1950</td>\n      <td>S</td>\n      <td>F</td>\n      <td>robin5@adventure-works.com</td>\n      <td>$100,000</td>\n      <td>3</td>\n      <td>Partial College</td>\n      <td>Management</td>\n      <td>Y</td>\n      <td>right_only</td>\n    </tr>\n    <tr>\n      <th>56050</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11441</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>MRS.</td>\n      <td>ERIKA</td>\n      <td>GOMEZ</td>\n      <td>6/12/1947</td>\n      <td>M</td>\n      <td>F</td>\n      <td>erika0@adventure-works.com</td>\n      <td>$90,000</td>\n      <td>5</td>\n      <td>Partial High School</td>\n      <td>Skilled Manual</td>\n      <td>N</td>\n      <td>right_only</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Exercise\n",
    "'''\n",
    "1. Find rows in the df created after right join that are only present in customer_df\n",
    "\n",
    "HINT : _merge can have the values - 'both', 'left_only', 'right_only'\n",
    "\n",
    "'''\n",
    "\n",
    "rows_tf = joined_df['_merge'] == 'right_only'\n",
    "\n",
    "filtered_df = joined_df[rows_tf]\n",
    "print(len(filtered_df))\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "7V5D9hcQvOte",
    "outputId": "4d0e09b3-07fc-4936-dd3c-6b011749519e"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 4.5 OUTER JOIN\n",
    "##########\n",
    "\n",
    "# Keep all the customer_key's from both sales and customer datasets and attmpt to match each of them (left + right join).\n",
    "# Look at the '_merge' column added by the argument 'indicator=True' to see which rows did not have a counterpart \n",
    "# in the other dataset.\n",
    "joined_df = pd.merge(   left=sales_df, right=customer_df, \n",
    "                        how='outer',\n",
    "                        left_on='customer_key', right_on='customer_key',\n",
    "                        suffixes=('_sales', '_cust'))\n",
    "print('The joined df has %d unique customer_key values' % len(joined_df['customer_key'].drop_duplicates()))\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "colab_type": "code",
    "id": "kOCnqpt4vOtg",
    "outputId": "9403ab30-3a9d-417b-aedf-d3ee9578c9df"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  order_number new_order_number\n0      SO45080     NEW--SO45080\n1      SO45079     NEW--SO45079\n2      SO45082     NEW--SO45082\n3      SO45081     NEW--SO45081\n4      SO45083     NEW--SO45083",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_number</th>\n      <th>new_order_number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SO45080</td>\n      <td>NEW--SO45080</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SO45079</td>\n      <td>NEW--SO45079</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SO45082</td>\n      <td>NEW--SO45082</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SO45081</td>\n      <td>NEW--SO45081</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SO45083</td>\n      <td>NEW--SO45083</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "########################\n",
    "# 5. ADVANCED STATISTICAL METHODS\n",
    "########################\n",
    "\n",
    "##########\n",
    "# 5.1 LAMBDA FUNCTIONS\n",
    "##########\n",
    "\n",
    "# Lambda functions are single-line functions that typically perform some time of data transformation \n",
    "# They can be used inside the .apply() function to very efficiently perform the same operation\n",
    "# on each row in a dataframe column\n",
    "\n",
    "# For example, the following lambda function is used to add 'NEW--' to the beginning of each value in the 'OrderNumber' column\n",
    "# The new values are stored in a new column named 'new_OrderNumber'\n",
    "sales_df['new_order_number'] = sales_df['order_number'].apply(func=(lambda x: 'NEW--' + str(x)))\n",
    "sales_df[['order_number', 'new_order_number']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  order_number order_date new_order_name_date\n0      SO45080   1/1/2015  SO45080 - 1/1/2015\n1      SO45079   1/1/2015  SO45079 - 1/1/2015\n2      SO45082   1/1/2015  SO45082 - 1/1/2015\n3      SO45081   1/1/2015  SO45081 - 1/1/2015\n4      SO45083   1/2/2015  SO45083 - 1/2/2015",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_number</th>\n      <th>order_date</th>\n      <th>new_order_name_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SO45080</td>\n      <td>1/1/2015</td>\n      <td>SO45080 - 1/1/2015</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SO45079</td>\n      <td>1/1/2015</td>\n      <td>SO45079 - 1/1/2015</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SO45082</td>\n      <td>1/1/2015</td>\n      <td>SO45082 - 1/1/2015</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SO45081</td>\n      <td>1/1/2015</td>\n      <td>SO45081 - 1/1/2015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SO45083</td>\n      <td>1/2/2015</td>\n      <td>SO45083 - 1/2/2015</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# We can also use lambda functions that act on multiple dataframe columns\n",
    "# Can you tell what the following function will do before executing the code?\n",
    "sales_df['new_order_name_date'] = sales_df.apply(lambda row: ' - '.join([str(row.order_number), str(row.order_date)]), axis=1)\n",
    "sales_df[['order_number', 'order_date', 'new_order_name_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NEW--foo--bar\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  order_number order_date     new_order_name_date\n0      SO45080   1/1/2015  NEW--SO45080--1/1/2015\n1      SO45079   1/1/2015  NEW--SO45079--1/1/2015\n2      SO45082   1/1/2015  NEW--SO45082--1/1/2015\n3      SO45081   1/1/2015  NEW--SO45081--1/1/2015\n4      SO45083   1/2/2015  NEW--SO45083--1/2/2015",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_number</th>\n      <th>order_date</th>\n      <th>new_order_name_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SO45080</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45080--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SO45079</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45079--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SO45082</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45082--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SO45081</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45081--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SO45083</td>\n      <td>1/2/2015</td>\n      <td>NEW--SO45083--1/2/2015</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# For more complex tranformations, its best to create a normal function that is called within the lambda function\n",
    "# Here, we define the function\n",
    "def concatenate_num_date(order_num, order_date):\n",
    "  concatenated = ''.join(['NEW--', str(order_num), '--', str(order_date)])\n",
    "  return(concatenated)\n",
    "\n",
    "# Let's look at what the function does on a single set of data points\n",
    "print(concatenate_num_date('foo', 'bar'))\n",
    "# Now we'll use the function in our lambda function\n",
    "sales_df['new_order_name_date'] = sales_df.apply(lambda row: concatenate_num_date(row.order_number, row.order_date), axis=1)\n",
    "sales_df[['order_number', 'order_date', 'new_order_name_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/usr/local/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n100%|██████████| 56046/56046 [00:02<00:00, 20272.30it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  order_number order_date     new_order_name_date\n0      SO45080   1/1/2015  NEW--SO45080--1/1/2015\n1      SO45079   1/1/2015  NEW--SO45079--1/1/2015\n2      SO45082   1/1/2015  NEW--SO45082--1/1/2015\n3      SO45081   1/1/2015  NEW--SO45081--1/1/2015\n4      SO45083   1/2/2015  NEW--SO45083--1/2/2015",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_number</th>\n      <th>order_date</th>\n      <th>new_order_name_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SO45080</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45080--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SO45079</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45079--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SO45082</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45082--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SO45081</td>\n      <td>1/1/2015</td>\n      <td>NEW--SO45081--1/1/2015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SO45083</td>\n      <td>1/2/2015</td>\n      <td>NEW--SO45083--1/2/2015</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# Bonus: Progress bar\n",
    "# Use tqdm's progress_apply to render a progress bar during complex / long processing\n",
    "from tqdm import tqdm # Import the function\n",
    "tqdm.pandas() # Start tqdm for Pandas operations\n",
    "\n",
    "# Now, we'll use .progress_apply() instead of the .apply() we have been using\n",
    "sales_df['new_order_name_date'] = sales_df.progress_apply(lambda row: concatenate_num_date(row.order_number, row.order_date), axis=1)\n",
    "sales_df[['order_number', 'order_date', 'new_order_name_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Expensive  product_cost\n0      False      413.1463\n1       True     2171.2942\n2      False     1898.0944\n3      False      413.1463\n4       True     2171.2942",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Expensive</th>\n      <th>product_cost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>False</td>\n      <td>413.1463</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>True</td>\n      <td>2171.2942</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>False</td>\n      <td>1898.0944</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>False</td>\n      <td>413.1463</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>True</td>\n      <td>2171.2942</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# Exercise\n",
    "'''\n",
    "Create a new row called \"Expensive\" with the values True or False. \n",
    "1. True if the product cost is >= 2000\n",
    "2. False if it is less than 2000\n",
    "'''\n",
    "\n",
    "sales_df.columns\n",
    "\n",
    "sales_df['Expensive'] = sales_df['product_cost'].apply(func = (lambda x: True if x >=2000 else False))\n",
    "\n",
    "sales_df[['Expensive', 'product_cost']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "AUmrVnFDvOti",
    "outputId": "26550b08-5254-4fd2-f4e2-e636bf07d9a5"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 5.2 CORRELATIONS\n",
    "##########\n",
    "\n",
    "# We can use the .corr() function to compute the pairwise correlation of columns, excluding NA/null values\n",
    "# By default, the function uses the standard Pearson correlation coefficient\n",
    "# But we can also choose to use the Kendall Tau coefficient (method=kendall) or the Spearman rank correlation (method=spearman)\n",
    "# We can also define our own correlation method function, and then use method=function_name\n",
    "# To learn more: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
    "\n",
    "# Here, we'll compute the correlation of all columns in the sales dataframe\n",
    "corr_matrix = sales_df.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly, it doesn't make much sense to compute correlations between every column\n",
    "# So let's build a new dataframe with only a few columns, then compute the column correlation again\n",
    "my_cols = sales_df[['order_quantity', 'product_cost', 'product_price']]\n",
    "corr_matrix = my_cols.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ypRSUvzYvOtk",
    "outputId": "16799ca4-19f2-4526-8c0e-ed43388a4f69"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 5.3 MATRIX MULTIPLICATIONS\n",
    "##########\n",
    "\n",
    "# To compute the innner product between two dataframes or series, we can use the .dot() function\n",
    "# Note: The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. \n",
    "#       In addition, the column names of DataFrame and the index of other must contain the same values, \n",
    "#       as they will be aligned prior to the multiplication.\n",
    "# Learn more: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dot.html \n",
    "\n",
    "# Here, we show a simple example of computing the dot product between two data series\n",
    "series1 = pd.Series([7, 5, 6, 4, 9]) \n",
    "series2 = pd.Series([1, 2, 3, 10, 2]) \n",
    "dot_prod = series1.dot(series2) \n",
    "dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "colab_type": "code",
    "id": "gzlxgOMFvOtn",
    "outputId": "f30fec0c-e0f5-498f-ac8c-f11bad6acee9"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# 6. TRANSFORMING DATAFRAMES\n",
    "########################\n",
    "\n",
    "# We can use the .pivot() function to create a new derived dataframe out of a given one \n",
    "# The function takes 3 arguements 'index', 'columns', and 'values' - each must be a column name in the original dataframe\n",
    "# When executed, the function will create a new dataframe, whose row and column indices are the unique values of the \n",
    "# respective parameters. The cell values of the new table are taken from column given as the 'values' parameter.\n",
    "\n",
    "# In this example, we'll be looking at new sales quantity for each brand for each day\n",
    "# First, we'll isolate the columns we need, then aggregate the new sales quantities\n",
    "\n",
    "my_cols = sales_df[['order_date', 'product_name', 'order_quantity']]\n",
    "\n",
    "# new_sales_df = my_cols.groupby(['order_date', 'product_name'], as_index = False).agg('sum')\n",
    "\n",
    "new_sales_df = my_cols.groupby(['order_date', 'product_name']).agg('sum').reset_index()\n",
    "new_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll create a new table with columns = ProductName, rows = OrderDate, and values = OrderQuantity\n",
    "result1 = new_sales_df.pivot(index='order_date', columns='product_name', values='order_quantity').fillna(0).astype(int)\n",
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .pivot_table() function is a generalization of .pivot() that can handle duplicate values for one pivoted index/column pair \n",
    "# Specifically, you can give .pivot_table() a list of aggregation functions using keyword argument aggfunc\n",
    "# The default aggfunc of .pivot_table() is numpy.mean\n",
    "\n",
    "# Let's do the same operation we just did, but using .pivot_table() this time\n",
    "result2 = sales_df.pivot_table(index='order_date', columns='product_name', values='order_quantity', aggfunc=np.sum, fill_value = 0)\n",
    "result2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the two results are the same, we can use the .equals() function\n",
    "equal_bool = result1.equals(result2)\n",
    "print('The two results are equal:', equal_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .transpose() function is fairly self-explanatory - we can use this function to \n",
    "# compute the transpose of a dataframe. Sometimes this comes in handy to get data ready for plotting.\n",
    "# For example, let's say we need our OrderQuantity pivot table to have columns = dates and rows = products\n",
    "transposed1 = result2.transpose()\n",
    "transposed1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use df.T to accomplish the same thing (The property T is an accessor to the .transpose() function)\n",
    "transposed2 = result2.T\n",
    "print('The two results are equal:', transposed1.equals(transposed2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 56046/56046 [00:00<00:00, 102358.54it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sales' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-868bb7a831cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msales_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnew_sale_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0magg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sales_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order_date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'product_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'revenue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sales' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "'''\n",
    "1. Create a new column called 'revenue' which is the product of 'product_price' and 'order_quantity' in a new df called 'new_sales_df'\n",
    "2. Create a new table with 'order_date' as the index, 'product_name' as column and the value as the aggregate of the revenue values for the product on a date (pivot vs pivot_table)\n",
    "3. Find the total revenue from AWC Logo Cap in 2016\n",
    "'''\n",
    "\n",
    "sales_df['revenue'] = sales_df.apply(lambda row: row['product_price'] * row['order_quantity'] , axis = 1)\n",
    "\n",
    "#sales_df['order_date'] = sales_df['order_date'].progress_apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "new_sale_df = sales_df\n",
    "\n",
    "agg_df = new_sales_df.groupby(['order_date', 'product_name'])['revenue'].agg('sum').reset_index()\n",
    "\n",
    "pivot_df = agg_df.pivot( index= 'order_date', columns = 'product_name', values = 'revenue').fillna(0)\n",
    "\n",
    "sum = pivot_df.loc['2016-01-01':'2017-01-01', 'AWC Logo Cap'].sum()\n",
    "\n",
    "new_sales_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUcMuLy8vOtq"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# 7. VISUALIZING DATA\n",
    "########################\n",
    "\n",
    "# Pandas dataframes work very well with another Python library - matplotlib\n",
    "# For more information about the matplotlib library, see:\n",
    "# https://matplotlib.org/\n",
    "# https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet\n",
    "\n",
    "# First, we need to import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZviAgJzvOts",
    "outputId": "ef8c38cc-66e5-4da2-dd8b-4c188de29ea2",
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  category_name  order_quantity\n0   Accessories           33607\n1         Bikes           13929\n2      Clothing            8510",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category_name</th>\n      <th>order_quantity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Accessories</td>\n      <td>33607</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bikes</td>\n      <td>13929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Clothing</td>\n      <td>8510</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "##########\n",
    "# 7.1 Bar Chart\n",
    "##########\n",
    "\n",
    "# In this example, display the order quantity for each category\n",
    "\n",
    "# First, we'll prepare the data\n",
    "data_to_plot = sales_df.groupby('category_name')['order_quantity'].agg('count')\n",
    "data_to_plot = data_to_plot.reset_index()\n",
    "data_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we'll make sure our plotting object is cleared\n",
    "plt.clf()\n",
    "\n",
    "# Then, we'll create a plot showing the order quantity for each category\n",
    "ax = data_to_plot.plot.bar(x = 'category_name', y = 'order_quantity')\n",
    "\n",
    "# Then, we'll set the plot title, x and y axis labels\n",
    "ax.set_title('Bar chart showing the order quantity of each category')\n",
    "ax.set_xlabel(\"Category\")\n",
    "ax.set_ylabel(\"Order Quantity\")\n",
    "\n",
    "# Then, we'll save the plot into a directory called 'plots/'\n",
    "plt.savefig('./plots/bar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EraKrV_mvOtt",
    "outputId": "142bf18e-adc7-4f53-ed8d-23cfc6c83283",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "object\n"
    }
   ],
   "source": [
    "##########\n",
    "# 7.2 TIME SERIES\n",
    "##########\n",
    "\n",
    "# In this example, we will plot the daily sales for Road Bikes, Mountain Bikes, Touring Bikes over time\n",
    "\n",
    "# First, let's look at the data type of our 'order_date' column (this column contains date information)\n",
    "print(sales_df['order_date'].dtypes) # Result is 'object' meaning the column contains strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 56046/56046 [00:14<00:00, 3819.29it/s]\n"
    }
   ],
   "source": [
    "# In order to plot the data over time, we need to convert the column from a string to a datetime type,\n",
    "# which can be plotted as a time series. Let's also convert the dates into month bins by setting each\n",
    "# day to the first of the month, in order to make the visualization easier to interpret.\n",
    "\n",
    "# We will use a lambda function to do this :)\n",
    "\n",
    "sales_df['order_date'] = sales_df['order_date'].progress_apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "datetime64[ns]\n"
    }
   ],
   "source": [
    "print(sales_df['order_date'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_df = sales_df[sales_df['category_name'] == 'Bikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  order_date subcategory_name  order_quantity\n0 2015-01-01       Road Bikes               1\n1 2015-01-01       Road Bikes               1\n2 2015-01-01   Mountain Bikes               1\n3 2015-01-01       Road Bikes               1\n4 2015-01-02       Road Bikes               1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_date</th>\n      <th>subcategory_name</th>\n      <th>order_quantity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-01</td>\n      <td>Road Bikes</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-01-01</td>\n      <td>Road Bikes</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2015-01-01</td>\n      <td>Mountain Bikes</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-01-01</td>\n      <td>Road Bikes</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015-01-02</td>\n      <td>Road Bikes</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Next, we'll keep only the columns we need\n",
    "filtered_df = bikes_df[['order_date', 'subcategory_name', 'order_quantity']]\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['subcategory_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll group first by 'order_date', then by 'subcategory_name', \n",
    "# and sum the only remaining column 'order_quantity'\n",
    "total_bikes_df = filtered_df.groupby(['order_date', 'subcategory_name']).agg('sum').reset_index()\n",
    "# total_sales_df = total_sales_df.sort_values(by = ['order_date'])\n",
    "# Let's look at what our data now looks like\n",
    "total_bikes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bikes_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to pivot the dataframe to get it in the right shape for plotting\n",
    "pivoted_sales_df = total_bikes_df.pivot_table(index='order_date', values='order_quantity', columns='subcategory_name')\n",
    "# Let's look at what our data now looks like\n",
    "pivoted_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_sales_df = pivoted_sales_df.fillna(0)\n",
    "pivoted_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we're ready to plot\n",
    "# We clear the plot area, plot our data on the axis, add labels, and save the figure\n",
    "plt.clf()\n",
    "ax = pivoted_sales_df.plot()\n",
    "ax.set_title('Total weekly sales for Road Bikes, Mountain Bikes, Touring Bikes')\n",
    "ax.set_xlabel(\"Week beginning\")\n",
    "ax.set_ylabel(\"Total sales\")\n",
    "plt.savefig('plots/time_series.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An easier to visualize way to plot this data\n",
    "\n",
    "pivoted_sales_df = pivoted_sales_df.cumsum(axis = 0)\n",
    "pivoted_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we're ready to plot\n",
    "# We clear the plot area, plot our data on the axis, add labels, and save the figure\n",
    "plt.clf()\n",
    "ax = pivoted_sales_df.plot()\n",
    "ax.set_title('Total weekly sales for Road Bikes, Mountain Bikes, Touring Bikes')\n",
    "ax.set_xlabel(\"Week beginning\")\n",
    "ax.set_ylabel(\"Total sales\")\n",
    "plt.savefig('plots/time_series.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kn9jlM6JvOty",
    "outputId": "4ca5426b-74b3-4f33-afd5-7e95f60796df"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 7.3 HEAT MAP\n",
    "##########\n",
    "\n",
    "# Another plotting option is a heat map \n",
    "\n",
    "# We import a supporting library and prepare the plot area\n",
    "import seaborn as sns\n",
    "plt.clf()\n",
    "plt.figure(figsize=(20,10)) # This explicity sets the size of the plot area (2x as wide as tall)\n",
    "ax = plt.axes()\n",
    "\n",
    "# Then we plot the data, add labels, and save the figure\n",
    "sns.heatmap(pivoted_sales_df, ax = ax)\n",
    "ax.set_title('Heatmap of weekly sales for Road Bikes, Mountain Bikes, Touring Bikes')\n",
    "ax.set_xlabel(\"Bike Type\")\n",
    "ax.set_ylabel(\"Week beginning\")\n",
    "plt.savefig('plots/heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HJYVuLBvOt1",
    "outputId": "799e3b71-835d-4f49-a55e-0994f982445e"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 7.4 SCATTER PLOT\n",
    "##########\n",
    "\n",
    "# Let's plot month vs total sales amount \n",
    "new_sales_df = sales_df\n",
    "new_sales_df['revenue'] = sales_df.apply(lambda x: x['order_quantity'] * x['product_price'], axis = 1)\n",
    "# new_sales_df['revenue'] = sales_df.apply(lambda x: , axis = 1)\n",
    "\n",
    "# Let's see if the month has an effect on the amount of revenue\n",
    "rev = new_sales_df.groupby(new_sales_df.order_date.dt.month).agg('revenue').sum().reset_index()\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(rev.order_date, rev.revenue, marker='o', linestyle='', ms=12, alpha = 0.5)\n",
    "plt.savefig('plots/scatter_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 7.5 SCATTER PLOT WITH NICER FORMATTING\n",
    "##########\n",
    "\n",
    "# Matplotlib allows for an extensive amount of formatting customization\n",
    "# Options include, but are certainly not limited to: graph title, axis titles, grid, axis range, legend position\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# And axis (data series) title\n",
    "ax.set_title('Monthly sales overview')\n",
    "# Make the grid visible\n",
    "ax.grid(True)\n",
    "# Set the x axis label and range\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_xlim((0, 15))\n",
    "# Set the y axis label and range\n",
    "ax.set_ylabel('Total sales value')\n",
    "ax.set_ylim((0, 5000000))\n",
    "\n",
    "ax.plot(rev.order_date, rev.revenue, marker='o', linestyle='', ms=12, alpha = 0.5)\n",
    "\n",
    "# Now we'll save the new version and compare to the previous one\n",
    "plt.savefig('plots/scatter_plot_new.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgsfqb6KvOt5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3011"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "########################\n",
    "# 8. ACTIVITIES\n",
    "########################\n",
    "\n",
    "customer_df = pd.read_csv('./data/customers_data.csv')\n",
    "\n",
    "# Let's switch to the customers dataset for the activities. First we have to turn the annual_income\n",
    "# column from a type string into a type integer\n",
    "def salary_transform(string):\n",
    "    s = string.replace('$', '')\n",
    "    s = s.replace(',', '')\n",
    "    return int(s)\n",
    "\n",
    "customer_df['annual_income'] = customer_df['annual_income'].apply(lambda x: salary_transform(x))\n",
    "\n",
    "##########\n",
    "# 8.1 ACTIVITY 1\n",
    "##########\n",
    "\n",
    "# Using the customer_df:\n",
    "# 1. Which 'occupation' has the HIGHEST MEAN and MEDIAN 'annual_income' value? And what are the values? \n",
    "# Hint: it's the same occupation.\n",
    "\n",
    "# 2. For the occupation you found in step 1, how many rows in customer_df does it appear in?\n",
    "\n",
    "###\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "customer_df.groupby('occupation')['annual_income'].agg(['mean','median']).reset_index()\n",
    "\n",
    "tf = customer_df['occupation'] == 'Management'\n",
    "\n",
    "len(customer_df[tf])\n",
    "###\n",
    "\n",
    "# Correct results for step 1: occupation = Management, mean = 92118.53, median = 90000\n",
    "# Correct results for step 2: 3011 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89h9-6crvOt9"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "# 8.2 ACTIVITY 2\n",
    "##########\n",
    "\n",
    "sales_df = pd.read_csv('./data/sales_data_all.csv')\n",
    "\n",
    "# Using the sales_df:\n",
    "# 1. Use a lambda function to compute ('product_price' - 'product_cost') for each row\n",
    "#    Then, sum all of the resulting values to compare with the correct answer below\n",
    "# 2. Use a lambda function and subsequent aggregation to compute how many more characters, \n",
    "#    on average (mean), the 'product_description' column has compared to the 'product_name' \n",
    "\n",
    "###\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(sales_df.apply(lambda x : x['product_price'] - x['product_cost'], axis = 1).sum())\n",
    "print(sales_df.apply(lambda x : len(x['product_description']) - len(x['product_name']), axis = 1).agg('mean'))\n",
    "\n",
    "###\n",
    "\n",
    "# Correct answer for step 1: 10268688.964499999\n",
    "# Correct answer for step 2: 55.49814438140099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "module6_data_analysis_in_pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}